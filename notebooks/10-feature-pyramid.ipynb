{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Pyramid\n",
    "\n",
    "![feature pyramid](https://lilianweng.github.io/lil-log/assets/images/featurized-image-pyramid.png)\n",
    "\n",
    "This is a more advanced approach.\n",
    "We can stride down and up _within a repeated block_.\n",
    "Skip connections prevent losing too much information.\n",
    "\n",
    "## Caveats\n",
    "\n",
    "As we are decreasing the size, increasing the size _and_ using skip connections this block is very sensitive to the image size.\n",
    "If the image size is not a multiple of 4 you will end up with rounding errors, breaking your model.\n",
    "Furthermore you can't go smaller than 1, so you cannot apply only this layer.\n",
    "\n",
    "These conditions end up enforcing a fixed image size on your network.\n",
    "This kind of limitation is why, in practice, image processing works over fixed size images.\n",
    "\n",
    "## Upsampling\n",
    "\n",
    "Finally, how do we scale up?\n",
    "We can scale up using an `nn.Upsample` layer.\n",
    "We use the same kind of scaling that is used in a paint program.\n",
    "You can use a convolution to scale up (called a deconvolution) however it is not a good idea.\n",
    "\n",
    "The upsample layer can upscale using a number of different scaling algorithms.\n",
    "`nearest` is the simplest (just copy the pixels) and `bilinear` is the one you're probably used to when editing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Upsample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipSequential(nn.Sequential):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_prime = super().forward(x)\n",
    "        return torch.cat([x, x_prime], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_block(in_channels: int, out_channels: int) -> nn.Module:\n",
    "    # outer -> middle -> inner -> middle -> outer\n",
    "    \n",
    "    # input_size: out_channels\n",
    "    # output_size: out_channels + out_channels\n",
    "    inner = SkipSequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "        nn.Upsample(scale_factor=2)\n",
    "    )\n",
    "    \n",
    "    # input_size: in_channels\n",
    "    # output_size: in_channels + out_channels + out_channels\n",
    "    middle = SkipSequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "        inner,\n",
    "        nn.Upsample(scale_factor=2)\n",
    "    )\n",
    "    \n",
    "    # input_size: in_channels\n",
    "    # output_size: out_channels\n",
    "    outer = nn.Sequential(\n",
    "        middle,\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels + out_channels + out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "    return outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Because this shrinks and expands, there is a point past which this will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid_block(3, 16)(torch.zeros(1, 3, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid_block(3, 16)(torch.zeros(1, 3, 4, 4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
