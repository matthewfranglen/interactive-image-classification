{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework\n",
    "\n",
    "In the previous workshop I proposed a simple framework:\n",
    "\n",
    " * An intro section\n",
    " * A repeated block\n",
    " * A classification section\n",
    "\n",
    "We should use that again now and then each of us can try out different techniques to see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "if not CUDA_AVAILABLE:\n",
    "    print(\"If you are running this on Google Colab then\")\n",
    "    print(\"Menu -> Runtime -> Change runtime type -> Hardware Accelerator -> GPU\")\n",
    "    print(\"Then try this again...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_image(image: torch.Tensor) -> PIL.Image:\n",
    "    # the rescaling also reverses the normalization (close enough)\n",
    "    image -= image.min()\n",
    "    image /= image.max()\n",
    "    return transforms.functional.to_pil_image(image.cpu(), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train: torch.utils.data.DataLoader,\n",
    "    valid: torch.utils.data.DataLoader,\n",
    "    epochs: int = 4,\n",
    "    lr: float = 0.001\n",
    ") -> None:\n",
    "    optimizer = optim.AdamW(params=model.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_batches = len(train)\n",
    "    train_loss = 0.\n",
    "\n",
    "    valid_batches = len(valid)\n",
    "    valid_loss = 0.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = eval_loss = 0.\n",
    "\n",
    "        for inputs, targets in tqdm(train, desc=f\"train {epoch}\"):\n",
    "            if CUDA_AVAILABLE:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss_value = loss(outputs, targets)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss_value.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(valid, desc=f\"valid {epoch}\"):\n",
    "                if CUDA_AVAILABLE:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss_value = loss(outputs, targets)\n",
    "                valid_loss += loss_value.item()\n",
    "        \n",
    "        # remember tensorboardx makes pretty graphs\n",
    "        train_loss /= train_batches\n",
    "        valid_loss /= valid_batches\n",
    "        print()\n",
    "        print(f\"train loss: {train_loss:.2e}\")\n",
    "        print(f\"valid loss: {valid_loss:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(\n",
    "    model: nn.Module,\n",
    "    valid: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    correct, incorrect = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(valid, desc=f\"validation\"):\n",
    "            if CUDA_AVAILABLE:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            matching = torch.eq(targets, outputs.argmax(dim=1)).sum().item()\n",
    "\n",
    "            correct += matching\n",
    "            incorrect += inputs.shape[0] - matching\n",
    "    \n",
    "    return correct / (correct + incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data\n",
    "\n",
    "Maybe you want to change the augmentation?\n",
    "You can just alter this bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.CIFAR10(\n",
    "    'data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transforms.Compose([\n",
    "        # This lets you randomly apply all the transformations in this list.\n",
    "        # The test is not once per transform, it either skips all transforms or applies all transforms.\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
    "        ]),\n",
    "        \n",
    "        # This is a combination of RandomApply and HorizontalFlip, by default has a 50% chance of flipping the image\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "\n",
    "        # We can only train with tensors, so we convert the image to a tensor.\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        # A very good thing to do is to normalize the tensors.\n",
    "        # This ensures the resulting tensors have a mean of 0 and a standard deviation of 1.\n",
    "        # For pre-existing datasets you can look up the normalization values, or you can calculate them like I did above.\n",
    "        transforms.Normalize(mean=(0.49139968, 0.48215841, 0.44653091), std=(0.24703223, 0.24348513, 0.26158784)),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = datasets.CIFAR10(\n",
    "    'data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # Must apply the same normalization!\n",
    "        transforms.Normalize(mean=(0.49139968, 0.48215841, 0.44653091), std=(0.24703223, 0.24348513, 0.26158784)),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Framework\n",
    "\n",
    "Here is the framework.\n",
    "The preparation and classification layers will be the same as the previous workshop.\n",
    "This is because the CIFAR-10 dataset has only 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_it_shrinks(train_dl: torch.utils.data.DataLoader, layer: nn.Module = nn.Conv2d(3, 3, kernel_size=(1, 1), stride=(2, 2))):\n",
    "    image, _ = next(iter(train_dl))\n",
    "    print(f\"Start: {image.shape}\")\n",
    "\n",
    "    for _ in range(5):\n",
    "        image = layer(image)\n",
    "        print(image.shape)\n",
    "\n",
    "how_it_shrinks(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above we can fit in 4 reductions, so if your repeated block has a single stride downwards then you can repeat that 4 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Intro\n",
    "\n",
    "I'm going to copy the intro block that was used before.\n",
    "Feel free to change this!\n",
    "\n",
    " * Maybe you want more convolutions?\n",
    " * Maybe change the activation?\n",
    " * Maybe add some more exotic layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intro() -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=8,\n",
    "            kernel_size=(7, 7),\n",
    "            padding=(4, 4),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "    # resnet18 also used a stride at this point, you could try that out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Classification\n",
    "\n",
    "I'm going to copy the classification block that was used before. Feel free to change this!\n",
    "\n",
    " * Maybe change AdaptiveMaxPool to something else?\n",
    " * Maybe more linear layers with activations?\n",
    "\n",
    "Remember that the maximum output of this is the chosen class, so adding something to the end that just scales will not change the results.\n",
    "It might change the training though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification() -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveMaxPool2d(output_size=(1,1)), # could try AdaptiveAvg\n",
    "        nn.Flatten(), # SNEAKY\n",
    "        nn.Linear(in_features=64, out_features=10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Repeating Block\n",
    "\n",
    "This is the best place to change. YOU MUST CHANGE THIS!\n",
    "\n",
    " * More layers?\n",
    " * Less layers?\n",
    " * Different layers?\n",
    " * More parameters to control the innards?\n",
    "\n",
    "Halving the size of the output is nice but not necessary.\n",
    "\n",
    "When applying a convolution remember to use a padding.\n",
    "The padding is half the kernel size, rounded down.\n",
    "This keeps the image the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeating_block(in_channels: int, out_channels: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Define Your Model!\n",
    "\n",
    "Again I'm copying the old model, feel free to change this (change it in the Train section though).\n",
    "\n",
    " * More features?\n",
    " * Less features?\n",
    " * etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is redefined in the train and evaluate so you can just copy that block\n",
    "# you can use this as a reference, change the one below\n",
    "\n",
    "raise ValueError(\"Change it in the cell below\")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    intro(),\n",
    "    repeating_block(8, 8), # intro increased to 8 already\n",
    "    repeating_block(8, 16), # image now 8x8\n",
    "    repeating_block(16, 32), # image now 4x4\n",
    "    repeating_block(32, 64), # image now 2x2\n",
    "    classification()\n",
    ")\n",
    "\n",
    "if CUDA_AVAILABLE:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train and Evaluate!\n",
    "\n",
    "Run this to see how well you did.\n",
    "Good luck!\n",
    "\n",
    "You can just copy the cell lots of times to try out different approaches and compare scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is here so that it is created fresh every time you evaluate it\n",
    "\n",
    "def intro() -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=8,\n",
    "            kernel_size=(7, 7),\n",
    "            padding=(4, 4),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "    # resnet18 also used a stride at this point, you could try that out\n",
    "\n",
    "def classification() -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveMaxPool2d(output_size=(1,1)), # could try AdaptiveAvg\n",
    "        nn.Flatten(), # SNEAKY\n",
    "        nn.Linear(in_features=64, out_features=10)\n",
    "    )\n",
    "\n",
    "def repeating_block(in_channels: int, out_channels: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    intro(),\n",
    "    repeating_block(8, 8), # intro increased to 8 already\n",
    "    repeating_block(8, 16), # image now 8x8\n",
    "    repeating_block(16, 32), # image now 4x4\n",
    "    repeating_block(32, 64), # image now 2x2\n",
    "    classification()\n",
    ")\n",
    "\n",
    "if CUDA_AVAILABLE:\n",
    "    model = model.cuda()\n",
    "\n",
    "train(model, train_dl, valid_dl, epochs=4, lr=0.001)\n",
    "score(model, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
